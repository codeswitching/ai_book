{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a1433f4-98ae-48a2-941d-378aa8d9b7d4",
   "metadata": {},
   "source": [
    "## 1. Perceptron Supporting Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cc176f-9ba3-4241-a539-74a551c49137",
   "metadata": {},
   "source": [
    "## 1.1 Perceptron Learning Ts vs Js - Step Through Manually\n",
    "Manually walk through the training procedure of Figure 1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a152510-aea8-4870-8f91-47e63a5bc66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123d1934-362d-4bec-a50f-090d6e4f8c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually code up examples\n",
    "examples=np.array([[[1,1,1,-1],   # T Shifted Left\n",
    "                    [-1,1,-1,-1],\n",
    "                    [-1,1,-1,-1],\n",
    "                    [-1,1,-1,-1]],\n",
    "                   [[-1,1,1,1],   # T Shifted Right\n",
    "                    [-1,-1,1,-1],\n",
    "                    [-1,-1,1,-1],\n",
    "                    [-1,-1,1,-1]],\n",
    "                   [[-1,-1,1,-1], # J Shifted Left\n",
    "                    [-1,-1,1,-1],\n",
    "                    [1,-1,1,-1],\n",
    "                    [1,1,1,-1]],\n",
    "                   [[-1,-1,-1,1], # J Shifted Right\n",
    "                    [-1,-1,-1,1],\n",
    "                    [-1,1,-1,1],\n",
    "                    [-1,1,1,1]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e7840f-af21-4a06-96eb-bf01706569d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(0,(6,6))\n",
    "for i in range(len(examples)):\n",
    "    fig.add_subplot(2,2,i+1)\n",
    "    plt.imshow(examples[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73daf3df-7940-4f6f-a717-d388394313e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup labels - we want our machine to output positive voltage to T shapes, \n",
    "# our first 2 examples are Ts - so we'll set these values to +1, our second 2 \n",
    "# examples are Js - so we'll set these to -1s\n",
    "y = np.array([1,1,-1,-1])\n",
    "\n",
    "# Reshape each example into a row, and add a 17th column for the bias term\n",
    "# Bias term is like a switch that is \"always on\" - it's an extra parameter that doesn't depend on our input and helps our model learn. \n",
    "X = np.hstack((examples.reshape(-1, 16), np.ones((len(y),1)))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39de133-06a9-427b-b7b3-3602cc34dabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecd76e4-fbda-40ee-9f66-6f130c8a6a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "X # 1 row for each example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfaf5f0-a815-4714-8fc7-72fb0092235a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights to zeros, this is equivalent to turning each knob to 12 o'clock\n",
    "w = np.zeros(17)\n",
    "lr = 1.0 # Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e7139e-f2a6-4625-a382-8c2a9468e022",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1 # Start with index 1, converges a little faster than starting at index 0\n",
    "\n",
    "# Compute perceptron output by taking dot product of example X and weights.\n",
    "yhat = np.dot(X[i], w) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1a98ec-6009-4cae-9273-0dc57d555145",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat, y[i] # Machine outputs 0, but we want it to output +1 (Case 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8f58ea-e269-4377-962d-32c2e76d9fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update weight following perceptron learning rule.\n",
    "# Adding our learning rate times our example is equivalent to turning up all our\n",
    "# dials that are switched on, and turning down all our dials that are switched off\n",
    "w = w + lr*X[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2af26d0-c65d-4123-9604-6eab76031115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# By adding our learning rate times our example now makes our weights look just like our first example.\n",
    "w[:16].reshape(4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59e603f-05cf-4add-bba8-c985ddf5cbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "i += 1 # Increment our counter i\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b20309e-9631-490f-94cf-57afac51b3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = np.dot(X[i], w) # Compute perceptron output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3705c20-cdb9-4cf5-826c-9bbee1967d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat, y[i] # Machine outputs +, but we want it to output - (Case 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5fd35e-07f1-4e70-a73a-227d3710931d",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = w - lr*X[i] # Machine output a +, but we wanted -, so *subtract* learning rate * examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f167105-2e72-4512-b7a3-f0a9003bb6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "w[:16].reshape(4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005cfc9f-d295-4bac-a7a0-f1fd15a5be6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "i += 1 # Increment our counter i\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cd1d84-35a8-4f30-b263-44c5d0450f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = np.dot(X[i], w) # Compute perceptron output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d892d776-2cad-4abf-9696-107a9289549d",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat, y[i] # Machine outputs +, but we want it to output - (Case 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d74f89-5b01-48be-b138-f86a1d4783df",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = w - lr*X[i] # Machine output a +, but we wanted -, so subtract learning rate * examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81da8a67-512a-4b8a-8465-2de2c12e2126",
   "metadata": {},
   "outputs": [],
   "source": [
    "w[:16].reshape(4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9dbc0e-a93c-47ee-ba10-b4ba78c6b310",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0 # We've reached the end of our examples (index 3, so start over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5c2bd6-8fe6-4bc9-a12e-5e579f82b73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = np.dot(X[i], w) # Compute perceptron output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2295fcd-84b6-4f19-992f-caeaeb3974be",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat, y[i] # Machine outputs +, and we want a +, so do not update weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0130b381-8b78-492b-add3-e66510f06b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cycle back through examples, print machine output and target output for each\n",
    "for i in range(4):\n",
    "    yhat=np.dot(X[i],w) # Compute perceptron output\n",
    "    print(yhat, y[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5afd736-bbed-44b3-b3d5-edc75803c8c3",
   "metadata": {},
   "source": [
    "Signs match in each case! Our perceptron is correctly classifying all examples. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987857fd-463d-401c-8953-c145f4e407b0",
   "metadata": {},
   "source": [
    "## 1.2 Perceptron Learning Ts vs Js - Step through in automated loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3753a8dc-a3ad-40cc-a458-19321317a7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manually Code up examples\n",
    "examples = np.array([[[1,1,1,-1], # T Shifted Left\n",
    "                    [-1,1,-1,-1],\n",
    "                    [-1,1,-1,-1],\n",
    "                    [-1,1,-1,-1]],\n",
    "                   [[-1,1,1,1],   # T Shifted Right\n",
    "                    [-1,-1,1,-1],\n",
    "                    [-1,-1,1,-1],\n",
    "                    [-1,-1,1,-1]],\n",
    "                   [[-1,-1,1,-1], # J Shifted Left\n",
    "                    [-1,-1,1,-1],\n",
    "                    [1,-1,1,-1],\n",
    "                    [1,1,1,-1]],\n",
    "                   [[-1,-1,-1,1], # J Shifted Right\n",
    "                    [-1,-1,-1,1],\n",
    "                    [-1,1,-1,1],\n",
    "                    [-1,1,1,1]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4eee9a7-f426-471d-bbf0-2a45fb44cefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(0,(6,6))\n",
    "for i in range(len(examples)):\n",
    "    fig.add_subplot(2,2,i+1)\n",
    "    plt.imshow(examples[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee47cd0-7a3d-452b-a1dc-062fafb8fc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup labels - we want our machine to output positive voltage to T shapes, \n",
    "# our first 2 examples are Ts - so we'll set these values to +1, our second to \n",
    "# examples are Js - so we'll set these to -1s\n",
    "y = np.array([1,1,-1,-1])\n",
    "\n",
    "# Reshape each example into a row, and add a 17th column for the bias term\n",
    "X = np.hstack((examples.reshape(-1, 16), np.ones((len(y),1)))) \n",
    "\n",
    "# Initialized weights to zeros, this is equivalent to turning each knob to 12 o'clock\n",
    "w = np.zeros(17)\n",
    "lr = 1.0 # Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67896aa2-0d69-43fc-b79b-b88c28b6cba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,10):   # 10 bc we're doing 10 training iterations\n",
    "    yhat = np.dot(X[i%len(y)], w) # % bc we have 4 samples but 10 iterations\n",
    "    if yhat<=0 and y[i%len(y)] > 0:      # Case 1\n",
    "        print(f\"output is {yhat} but we want it to be {y[i%len(y)]}, updating weights.\")\n",
    "        w = w + lr*X[i%len(y)] \n",
    "    elif yhat > 0 and y[i % len(y)]<=0:  # Case 2\n",
    "        print(f\"output is {yhat} but we want it to be {y[i%len(y)]}, updating weights.\")\n",
    "        w = w - lr*X[i%len(y)] \n",
    "    else: \n",
    "        print(f\"output is {yhat}, which has the same sign as our target {y[i%len(y)]}, \"\n",
    "              f\"machine is correct, not updating weights.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfb79e5-0661-46d3-8d9f-88364baf8780",
   "metadata": {},
   "source": [
    "## 1.3 Two input perceptron - solvable case (OR gate)\n",
    "Replicates results of Figure 1.10, Task 1\n",
    "(baby perceptron learning AND gate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53111d11-b228-4385-aee4-7c46e4221a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = np.array([[[-1,-1]],\n",
    "             [[-1,1]],\n",
    "             [[1,-1]],\n",
    "             [[1,1]]])\n",
    "\n",
    "y = np.array([-1,1,1,1]) # we want our machine to output + when either or both switches are on\n",
    "\n",
    "# Reshape each example into a row, and add a 3rd column for the bias term\n",
    "X = np.hstack((examples.reshape(-1, 2), np.ones((len(y),1)))) \n",
    "\n",
    "# Initialized weights to zeros, this is equivalent to turning each knob to 12 o'clock\n",
    "w = np.zeros(3)\n",
    "lr = 1.0 # Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0b5d21-8929-4ab3-8c57-5d1f371ddbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795a85d6-8ec7-44ea-837b-bb7bd1db9e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "w, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666da94c-f61c-446e-b4db-d1c67e400de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 12): # Starting at index 1 instead of 0 results are a little more clear this way.\n",
    "    yhat = np.dot(X[i%len(y)],w) \n",
    "    print(f\"step: {i}, current example: {X[i%len(y)][:2]}, current weights = {w}\")\n",
    "    if yhat<=0 and y[i%len(y)] > 0:    # Case 1\n",
    "        print(f\"output is {yhat} but we want it to be {y[i%len(y)]}, updating weights.\")\n",
    "        w = w + lr*X[i%len(y)] \n",
    "    elif yhat > 0 and y[i%len(y)]<=0:  # Case 2\n",
    "        print(f\"output is {yhat} but we want it to be {y[i%len(y)]}, updating weights.\")\n",
    "        w = w - lr*X[i%len(y)] \n",
    "    else: \n",
    "        print(f\"output is {yhat}, which has the same sign as our target {y[i%len(y)]}, \" \n",
    "              f\"machine is correct, not updating weights.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d91fcee-2eec-476d-b733-c86b3bdfca90",
   "metadata": {},
   "source": [
    "## 1.4 - Two input perceptron - unsolvable XOR case\n",
    "Replicates Results of Figure 1.10, Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369656ff-2395-4d1c-bdf9-9f7a0d0dbcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = np.array([[[-1,-1]],\n",
    "             [[-1,1]],\n",
    "             [[1,-1]],\n",
    "             [[1,1]]])\n",
    "\n",
    "y = np.array([-1,1,1,-1]) # Machine should output + when either, but not both switches are on\n",
    "\n",
    "# Reshape each example into a row, and add a 3rd column for the bias term\n",
    "X = np.hstack((examples.reshape(-1, 2), np.ones((len(y),1)))) \n",
    "\n",
    "# Initialized weights to zeros, this is equivalent to turning each knob to 12 o'clock\n",
    "w = np.zeros(3)\n",
    "lr = 1.0 #Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f00246-2347-49dd-8809-3654ef627cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d69307-1850-41d6-ae20-9c7d135f59b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "w, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e48d31a-aba0-4795-8cfc-5a45b2bec8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 14): # Starting at index 1, instead of 0, results are a little more clear this way.\n",
    "    yhat = np.dot(X[i%len(y)], w) \n",
    "    print(f\"step: {i}, current example: {X[i%len(y)][:2]}, current weights = {w}\")\n",
    "    if yhat<=0 and y[i%len(y)] > 0: \n",
    "        print(f\"output is {yhat} but we want it to be {y[i%len(y)]}, updating weights.\")\n",
    "        w = w + lr*X[i%len(y)] \n",
    "    elif yhat > 0 and y[i%len(y)]<=0: \n",
    "        print(f\"output is {yhat} but we want it to be {y[i%len(y)]}, updating weights.\")\n",
    "        w = w - lr*X[i%len(y)] \n",
    "    else: \n",
    "        print(f\"output is {yhat}, which has the same sign as our target {y[i%len(y)]}, \\\n",
    "              machine is correct, not updating weights.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc0065c-5e66-41aa-8461-072e1566a435",
   "metadata": {},
   "source": [
    "Note that our weights are stuck in a loop!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca89eadb-831c-4652-b15a-f579bfdf1996",
   "metadata": {},
   "source": [
    "## 1.6 Compute Perceptron Error Across a Range of Values\n",
    "Reproduces bowl-shaped error surface in Figure 1.24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064c1354-4e65-4f90-96d5-cb7c8fa757aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Linearly-separable case\n",
    "X = np.array([[-1, -1], [-1, 1], [1, -1], [1, 1]])\n",
    "y = np.array([[-1], [1], [1], [1]])  # AND operation\n",
    "w0_range = np.arange(-1.9, 2.0, 0.2) \n",
    "w1_range = np.arange(-1.9, 2.0, 0.2)\n",
    "b = 1   # Bias term\n",
    "\n",
    "# Initialize lists to store results\n",
    "w0_points = []\n",
    "w1_points = []\n",
    "error_points = []\n",
    "\n",
    "# Compute error for each weight combination\n",
    "for w0 in w0_range:\n",
    "    for w1 in w1_range:\n",
    "        yhat = X[:,0]*w0 + X[:,1]*w1 + b  # Compute all 4 yhats at once\n",
    "        error = np.mean((y.ravel() - yhat)**2)  # Mean Squared Error\n",
    "        \n",
    "        # Store the results\n",
    "        w0_points.append(w0)\n",
    "        w1_points.append(w1)\n",
    "        error_points.append(error)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "w0_points = np.array(w0_points)\n",
    "w1_points = np.array(w1_points)\n",
    "error_points = np.array(error_points)\n",
    "\n",
    "# Create 3D scatter plot\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Create scatter plot with color mapping\n",
    "scatter = ax.scatter(w0_points, w1_points, error_points, \n",
    "                    c = error_points, cmap = 'viridis',  # color by error value\n",
    "                    alpha = 0.6, s = 20)\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel('Weight w0')\n",
    "ax.set_ylabel('Weight w1')\n",
    "ax.set_zlabel('Mean Squared Error')\n",
    "ax.set_title('Perceptron Error Surface\\n(Linearly Separable Case)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d479ee-cce8-4120-92f7-5f576759fad8",
   "metadata": {},
   "source": [
    "## 1.7 Train Small Network to Solve XOR Using LMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a09cdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# XOR dataset\n",
    "X = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n",
    "y = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)\n",
    "\n",
    "# Simple 2-layer network: 2 inputs -> 2 hidden -> 1 output (similar to Fig. 1.30)\n",
    "class XORNet(nn.Module):\n",
    "    def __init__(self):                   # Define network layers\n",
    "        super(XORNet, self).__init__()    # Initialize parent class\n",
    "        self.hidden = nn.Linear(2, 2)     # Hidden layer with 2 neurons\n",
    "        self.output = nn.Linear(2, 1)     # Output layer\n",
    "        self.sigmoid = nn.Sigmoid()       # Sigmoid activation function\n",
    "    \n",
    "    def forward(self, x):                 # Define feed forward pass\n",
    "        x = self.sigmoid(self.hidden(x))  # Hidden layer with activation\n",
    "        x = self.sigmoid(self.output(x))  # Output layer with activation\n",
    "        return x\n",
    "\n",
    "# Initialize network, loss, and optimizer\n",
    "# Weights are chosen randomly, does not always converge - networks with more\n",
    "# hidden units will converge more often\n",
    "model = XORNet()\n",
    "\n",
    "# Same squared error that Widrow and Hoff used, just taking the average across all 4 examples\n",
    "# This is known as \"batch\" or \"minibatch\" gradient descent.\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Using the Adam optimizer here instead of vanilla SGD, SGD gets stuck when model is only 2 neurons wide. \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(4000):         # Train for 4000 epochs\n",
    "    optimizer.zero_grad()         # Initialize the gradients from the previous epoch\n",
    "    outputs = model(X)            # Compute model outputs\n",
    "    loss = criterion(outputs, y)  # Compute LMS loss\n",
    "    loss.backward()               # Compute gradients\n",
    "    optimizer.step()              # Update weights\n",
    "    \n",
    "    if epoch % 1000 == 0:         # Print loss every 1000 epochs\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item():.4f}')\n",
    "\n",
    "# Test the network\n",
    "print(\"\\nResults:\")\n",
    "with torch.no_grad():\n",
    "    for i in range(len(X)):        # Test each example\n",
    "        output = model(X[i:i+1])   # Get model output\n",
    "        print(f\"Input: {X[i].numpy()}, Target: {y[i].item()}, Output: {output.item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
